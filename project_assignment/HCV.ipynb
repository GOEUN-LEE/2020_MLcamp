{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HCV.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"PkKhm9-Q4JmY","colab_type":"code","outputId":"b7b4eaf9-9d21-402a-ca90-11cbd9c77241","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1579431631447,"user_tz":-540,"elapsed":22002,"user":{"displayName":"이고은학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD7Hhpz51CZiAt-pFqVW_whashy8di01qX2cWMt=s64","userId":"09774315254356044053"}}},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Io13NmFnJMxc","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","import pandas as pd\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.utils import shuffle "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWNt4hqINm5w","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class FFNet(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_classes):\n","    super(FFNet, self).__init__()\n","    self.fc1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(hidden_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.fc3 = nn.Linear(hidden_size, num_classes)\n","    #self.relu = nn.ReLU()\n","    #self.fc4 = nn.Linear(hidden_size, hidden_size)\n","    #self.relu = nn.ReLU()\n","    #self.fc5 = nn.Linear(hidden_size, hidden_size)\n","    #self.relu = nn.ReLU()\n","    #self.fc6 = nn.Linear(hidden_size, num_classes)\n","\n","  def forward(self, x):\n","    out = self.fc1(x)\n","    out = self.relu(out)\n","    out = self.fc2(out)\n","    out = self.relu(out)\n","    out = self.fc3(out)\n","    #out = self.relu(out)\n","    #out = self.fc4(out)\n","    #out = self.relu(out)\n","    #out = self.fc5(out)\n","    #out = self.relu(out)\n","    #out = self.fc6(out)\n","    return out \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qj1e39X-T1v0","colab_type":"code","colab":{}},"source":["def train_ffnet(model, train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (attr, labels, _) in enumerate(train_loader):\n","      attr = attr.reshape(-1, 27).to(device)\n","      labels = labels.to(device)\n","\n","      outputs = model(attr)\n","      loss = criterion(outputs, labels)\n","\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      if(i+1) % 100 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","        \n","def test_ffnet(model, test_loader):\n","  with torch.no_grad():\n","    correct=0\n","    total =0\n","    for attr, labels, _ in test_loader:\n","      attr = attr.reshape(-1, 27).to(device)\n","      labels = labels.to(device)\n","      outputs = model(attr)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","      print(predicted, labels)\n","    print('Accuracy of the network on the 1000 test instances: {}%'.format(100*correct/total))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4og_SJku8IUe","colab_type":"code","outputId":"0f51cb56-8f70-4f0f-d4a0-9a55f4ec2718","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1579431683377,"user_tz":-540,"elapsed":2390,"user":{"displayName":"이고은학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD7Hhpz51CZiAt-pFqVW_whashy8di01qX2cWMt=s64","userId":"09774315254356044053"}}},"source":["\"\"\"\n","class CustomDataset(torch.utils.data.Dataset): \n","  # Initialize your data, download, etc. \n","  def __init__(self, test): \n","    data = pd.read_csv('/content/drive/My Drive/2020-Winter/ML-Camp /HCV-Egy-Data.csv')\n","    data = data.to_numpy()\n","    data = shuffle(data, random_state=777)\n","    data = torch.from_numpy(data)\n","    if test == False: \n","      X_tr = data[0:1000, 0:27]\n","      y1_tr = data[0:1000, 29]\n","      y2_tr = data[0:1000, 28]\n","      #normalizer = MinMaxScaler(feature_range=(0,1))\n","      #normalizer.fit(X_tr)\n","      #X_tr_normalized = normalizer.transform(X_tr)\n","\n","      self.x_data = X_tr\n","      #data[0:1000, 0:27] \n","      self.y1_data = y1_tr \n","      self.y2_data = y2_tr  \n","      #data[0:1000, 27:29]\n","      self.len = data[0:1000,:].shape[0]\n","    \n","    elif test == True:\n","      X_ts = data[1000:, 0:27]\n","      y1_ts = data[1000:, 29]\n","      y2_ts = data[1000:, 28]\n","      normalizer = MinMaxScaler(feature_range=(0,1))\n","      normalizer.fit(X_ts)\n","      X_ts_normalized = normalizer.transform(X_ts) \n","      \n","      self.x_data = X_ts_normalized \n","      self.y1_data = y1_ts \n","      self.y2_data = y2_ts \n","      self.len = data[1000:,:].shape[0]\n","    \n","    else:\n","      print(\"Error on train\")\n","      return 0;  \n","  \n","  def __getitem__(self, index): \n","    return self.x_data[index], self.y1_data[index], self.y2_data[index]\n","   \n","  def __len__(self): \n","    return self.len\n","\"\"\""],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass CustomDataset(torch.utils.data.Dataset): \\n  # Initialize your data, download, etc. \\n  def __init__(self, test): \\n    data = pd.read_csv(\\'/content/drive/My Drive/2020-Winter/ML-Camp /HCV-Egy-Data.csv\\')\\n    data = data.to_numpy()\\n    data = shuffle(data, random_state=777)\\n    data = torch.from_numpy(data)\\n    if test == False: \\n      X_tr = data[0:1000, 0:27]\\n      y1_tr = data[0:1000, 29]\\n      y2_tr = data[0:1000, 28]\\n      #normalizer = MinMaxScaler(feature_range=(0,1))\\n      #normalizer.fit(X_tr)\\n      #X_tr_normalized = normalizer.transform(X_tr)\\n\\n      self.x_data = X_tr\\n      #data[0:1000, 0:27] \\n      self.y1_data = y1_tr \\n      self.y2_data = y2_tr  \\n      #data[0:1000, 27:29]\\n      self.len = data[0:1000,:].shape[0]\\n    \\n    elif test == True:\\n      X_ts = data[1000:, 0:27]\\n      y1_ts = data[1000:, 29]\\n      y2_ts = data[1000:, 28]\\n      normalizer = MinMaxScaler(feature_range=(0,1))\\n      normalizer.fit(X_ts)\\n      X_ts_normalized = normalizer.transform(X_ts) \\n      \\n      self.x_data = X_ts_normalized \\n      self.y1_data = y1_ts \\n      self.y2_data = y2_ts \\n      self.len = data[1000:,:].shape[0]\\n    \\n    else:\\n      print(\"Error on train\")\\n      return 0;  \\n  \\n  def __getitem__(self, index): \\n    return self.x_data[index], self.y1_data[index], self.y2_data[index]\\n   \\n  def __len__(self): \\n    return self.len\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"6HXcjZBhNTwJ","colab_type":"code","outputId":"c95a0c97-1a4e-4af0-c0ff-9844fb35dcb6","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","input_size = 27\n","num_classes = 16 \n","batch_size = 10\n","hidden_size = 23\n","learning_rate = 0.001\n","\n","custom_dataset_train = CustomDataset(test=False)\n","custom_dataset_test = CustomDataset(test=True)\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset=custom_dataset_train,\n","                                           batch_size=len(custom_dataset_train),\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=custom_dataset_test,\n","                                           batch_size=10,\n","                                           shuffle=False)\n","\n","X_tr = next(iter(train_loader))[0].float().numpy()\n","y_tr = next(iter(train_loader))[1].long().numpy()\n","X_ts = next(iter(train_loader))[0].float().numpy()\n","y_ts = next(iter(train_loader))[1].long().numpy()\n","\n","#X_tr = X_tr.astype('float64')\n","#y_tr = y_tr.astype(int)\n","#X_ts = X_ts.astype('float64') \n","#y_ts = y_ts.astype(int)\n","\n","X_tr = X_tr.reshape(-1, input_size)\n","X_ts = X_ts.reshape(-1, input_size) \n","print(X_tr)\n","print(y_tr)\n","print(X_ts.shape)\n","print(y_ts.shape)\n","\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ninput_size = 27\\nnum_classes = 16 \\nbatch_size = 10\\nhidden_size = 23\\nlearning_rate = 0.001\\n\\ncustom_dataset_train = CustomDataset(test=False)\\ncustom_dataset_test = CustomDataset(test=True)\\n\\n\\ntrain_loader = torch.utils.data.DataLoader(dataset=custom_dataset_train,\\n                                           batch_size=len(custom_dataset_train),\\n                                           shuffle=True)\\n\\ntest_loader = torch.utils.data.DataLoader(dataset=custom_dataset_test,\\n                                           batch_size=10,\\n                                           shuffle=False)\\n\\nX_tr = next(iter(train_loader))[0].float().numpy()\\ny_tr = next(iter(train_loader))[1].long().numpy()\\nX_ts = next(iter(train_loader))[0].float().numpy()\\ny_ts = next(iter(train_loader))[1].long().numpy()\\n\\n#X_tr = X_tr.astype('float64')\\n#y_tr = y_tr.astype(int)\\n#X_ts = X_ts.astype('float64') \\n#y_ts = y_ts.astype(int)\\n\\nX_tr = X_tr.reshape(-1, input_size)\\nX_ts = X_ts.reshape(-1, input_size) \\nprint(X_tr)\\nprint(y_tr)\\nprint(X_ts.shape)\\nprint(y_ts.shape)\\n\\n\""]},"metadata":{"tags":[]},"execution_count":287}]},{"cell_type":"code","metadata":{"id":"QepswEQ2EkkS","colab_type":"code","outputId":"9e9dd2c9-3ba2-4476-f437-4c2228043951","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["!pip install skorch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: skorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.8.6)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from skorch) (1.17.5)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from skorch) (0.22.1)\n","Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.6/dist-packages (from skorch) (4.28.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->skorch) (0.14.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"en4qdEddT5eN","colab_type":"code","outputId":"285a25a3-1930-4e05-ae51-f019efc9cad6","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["\"\"\"\n","from skorch import NeuralNetClassifier \n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","input_size = 27\n","num_classes = 16 \n","batch_size = 100 \n","hidden_size = 23\n","\n","net = NeuralNetClassifier(FFNet(input_size,hidden_size,num_classes), \n","                          criterion = nn.CrossEntropyLoss,\n","                          optimizer = torch.optim.Adam,\n","                          iterator_train__shuffle=True)\n","\n","params = { \n","    'lr':[0.01, 0.001, 0.0001],\n","    'max_epochs':[5,10,15],\n","    'module__input_size': [input_size],\n","    'module__hidden_size': [hidden_size],\n","    'module__num_classes': [num_classes],\n","}\n","\n","gridsearch = GridSearchCV(net, params, cv=3, scoring='accuracy')\n","\n","gridsearch.fit(X_tr, y_tr)\n","print(gridsearch.best_score_, gridsearch.best_params_)\n","\n","best_net = gridsearch.best_estimator_\n","y_pred = best_net.predict(X_ts)\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfrom skorch import NeuralNetClassifier \\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import accuracy_score\\n\\ninput_size = 27\\nnum_classes = 16 \\nbatch_size = 100 \\nhidden_size = 23\\n\\nnet = NeuralNetClassifier(FFNet(input_size,hidden_size,num_classes), \\n                          criterion = nn.CrossEntropyLoss,\\n                          optimizer = torch.optim.Adam,\\n                          iterator_train__shuffle=True)\\n\\nparams = { \\n    'lr':[0.01, 0.001, 0.0001],\\n    'max_epochs':[5,10,15],\\n    'module__input_size': [input_size],\\n    'module__hidden_size': [hidden_size],\\n    'module__num_classes': [num_classes],\\n}\\n\\ngridsearch = GridSearchCV(net, params, cv=3, scoring='accuracy')\\n\\ngridsearch.fit(X_tr, y_tr)\\nprint(gridsearch.best_score_, gridsearch.best_params_)\\n\\nbest_net = gridsearch.best_estimator_\\ny_pred = best_net.predict(X_ts)\\n\""]},"metadata":{"tags":[]},"execution_count":289}]},{"cell_type":"code","metadata":{"id":"DKKbS0kGeXgP","colab_type":"code","colab":{}},"source":["class CustomDataset(torch.utils.data.Dataset): \n","  # Initialize your data, download, etc. \n","  def __init__(self, test): \n","    data = pd.read_csv('/content/drive/My Drive/2020winter-MLcamp/content/HCV-Egy-Data.csv')\n","    data = data.to_numpy()\n","    data = shuffle(data, random_state=777)\n","    data = torch.from_numpy(data)\n","    if test == False: \n","      X_tr = data[0:1000, 0:27]\n","      y1_tr = data[0:1000, 27]\n","      y2_tr = data[0:1000, 28]\n","      #normalizer = MinMaxScaler(feature_range=(0,1))\n","      #0,2,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26\n","      #normalizer.fit(X_tr)\n","      #X_tr_normalized = normalizer.transform(X_tr)\n","      #X_tr_normalized = torch.from_numpy(X_tr_normalized)\n","      #X_tr_normalized = X_tr_normalized.float()\n","\n","      self.x_data = X_tr.float()\n","      #data[0:1000, 0:27] \n","      self.y1_data = y1_tr \n","      self.y2_data = y2_tr  \n","      #data[0:1000, 27:29]\n","      self.len = data[0:1000,:].shape[0]\n","    \n","    elif test == True:\n","      X_ts = data[1000:, 0:27]\n","      y1_ts = data[1000:, 27]\n","      y2_ts = data[1000:, 28]\n","      #normalizer = MinMaxScaler(feature_range=(0,1))\n","      #normalizer.fit(X_ts)\n","      #X_ts_normalized = normalizer.transform(X_ts) \n","      #X_ts_normalized = torch.from_numpy(X_ts_normalized)\n","      #X_ts_normalized = X_ts_normalized.float()\n","      \n","      self.x_data = X_ts.float() \n","      self.y1_data = y1_ts \n","      self.y2_data = y2_ts \n","      self.len = data[1000:,:].shape[0]\n","    \n","    else:\n","      print(\"Error on train\")\n","      return 0;  \n","  \n","  def __getitem__(self, index): \n","    return self.x_data[index], self.y1_data[index], self.y2_data[index]\n","   \n","  def __len__(self): \n","    return self.len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rTpqHBEryLH","colab_type":"code","colab":{}},"source":["input_size = 27\n","num_classes = 16 \n","batch_size = 15\n","hidden_size = 23\n","learning_rate = 0.0001\n","\n","custom_dataset_train = CustomDataset(test=False)\n","custom_dataset_test = CustomDataset(test=True)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=custom_dataset_train,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=custom_dataset_test,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","# for a, (i,j,k) in enumerate(train_loader):\n","#   print(i,j)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nH5a5TVprNOR","colab_type":"code","outputId":"8d2671cf-7f86-4615-d18d-d291e2d16eec","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1579431854067,"user_tz":-540,"elapsed":897,"user":{"displayName":"이고은학부생","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD7Hhpz51CZiAt-pFqVW_whashy8di01qX2cWMt=s64","userId":"09774315254356044053"}}},"source":["model = FFNet(input_size, hidden_size, num_classes).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_ffnet(model, train_loader, num_epochs=10)\n","test_ffnet(model, test_loader)"],"execution_count":20,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-259835cfe133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_ffnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_ffnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-df92aa59ef19>\u001b[0m in \u001b[0;36mtrain_ffnet\u001b[0;34m(model, train_loader, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:97"]}]},{"cell_type":"code","metadata":{"id":"-fxRHbAOVR2C","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}